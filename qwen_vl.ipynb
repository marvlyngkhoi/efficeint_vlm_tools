{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44e2d112-c763-467d-8811-df2bbdfba24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b86f581-3dc6-4426-9eee-d69765c8df97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "# URL of the image\n",
    "image_url = \"https://huggingface.co/spaces/merve/chameleon-7b/resolve/main/bee.jpg\"\n",
    "\n",
    "# Download the image\n",
    "response = requests.get(image_url)\n",
    "if response.status_code == 200:\n",
    "    img = Image.open(BytesIO(response.content))\n",
    "\n",
    "    # Resize to 256x256\n",
    "    img_resized = img.resize((256, 256))\n",
    "\n",
    "#     # Save the resized image\n",
    "#     img_resized.save(\"bee_resized.jpg\")\n",
    "#     print(\"Image resized and saved as bee_resized.jpg\")\n",
    "# else:\n",
    "#     print(\"Failed to download the image\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0837c308-da4e-4c54-a463-705f213de69c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Qwen2VLRotaryEmbedding` can now be fully parameterized by passing the model config through the `config` argument. All other arguments will be removed in v4.46\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5eb7cb673c374a5aa4dd44c7dee11fce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "\n",
    "# default: Load the model on the available device(s)\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-2B-Instruct\", torch_dtype=\"auto\", device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# We recommend enabling flash_attention_2 for better acceleration and memory saving, especially in multi-image and video scenarios.\n",
    "# model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "#     \"Qwen/Qwen2-VL-2B-Instruct\",\n",
    "#     torch_dtype=torch.bfloat16,\n",
    "#     attn_implementation=\"flash_attention_2\",\n",
    "#     device_map=\"auto\",\n",
    "# )\n",
    "\n",
    "# default processer\n",
    "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-2B-Instruct\")\n",
    "\n",
    "# The default range for the number of visual tokens per image in the model is 4-16384. You can set min_pixels and max_pixels according to your needs, such as a token count range of 256-1280, to balance speed and memory usage.\n",
    "# min_pixels = 256*28*28\n",
    "# max_pixels = 1280*28*28\n",
    "# processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-2B-Instruct\", min_pixels=min_pixels, max_pixels=max_pixels)\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"image\",\n",
    "                \"image\": img_resized#\"bee_resized.jpg\",\n",
    "            },\n",
    "            {\"type\": \"text\", \"text\": \"Describe this image.\"},\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "\n",
    "# Preparation for inference\n",
    "text = processor.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "image_inputs, video_inputs = process_vision_info(messages)\n",
    "inputs = processor(\n",
    "    text=[text],\n",
    "    images=image_inputs,\n",
    "    videos=video_inputs,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "inputs = inputs.to(\"cuda\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3d80fcf-5f8a-424e-90a4-061c69c46c9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The image depicts a close-up of a bee on a pink flower. The bee is actively feeding on the flower, which appears to be a type of cosmos or similar flower. The background is blurred, focusing attention on the bee and the flower. The overall scene suggests a natural setting, possibly a garden or a wildflower meadow.']\n"
     ]
    }
   ],
   "source": [
    "# Inference: Generation of the output\n",
    "generated_ids = model.generate(**inputs, max_new_tokens=128)\n",
    "generated_ids_trimmed = [\n",
    "    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "]\n",
    "output_text = processor.batch_decode(\n",
    "    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    ")\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c6f3f23-8367-4f27-a687-498766185df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm_press import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d872b05-5237-48a0-915e-23f59f9cacbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "press = KnormPress(0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "77cb1c0a-2f2e-46e7-8bda-ddfc4b83ffdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"The image depicts a close-up view of a bee or a similar insect on a flower. The insect is positioned on the petals of a flower, likely collecting nectar or pollen. The background is blurred, focusing attention on the insect and the flower. The colors in the image are natural and vibrant, with the green of the leaves and the pink or purple hues of the flowers contrasting nicely. The overall scene suggests a natural, outdoor setting, possibly in a garden or a park. The insect's presence indicates that the flower is in bloom and providing food for pollinators.\"]\n"
     ]
    }
   ],
   "source": [
    "# Inference: Generation of the output\n",
    "with torch.no_grad(),press(model):\n",
    "    generated_ids_pressed = model.generate(**inputs, max_new_tokens=128)\n",
    "    \n",
    "generated_ids_trimmed_pressed = [\n",
    "    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids_pressed)\n",
    "]\n",
    "output_text_pressed = processor.batch_decode(\n",
    "    generated_ids_trimmed_pressed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    ")\n",
    "print(output_text_pressed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db6cb85-6eb4-475f-993c-ee65f81df28b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:finbot]",
   "language": "python",
   "name": "conda-env-finbot-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
