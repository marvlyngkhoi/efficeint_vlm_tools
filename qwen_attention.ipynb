{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44e2d112-c763-467d-8811-df2bbdfba24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='6'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b86f581-3dc6-4426-9eee-d69765c8df97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "# URL of the image\n",
    "image_url = \"https://huggingface.co/spaces/merve/chameleon-7b/resolve/main/bee.jpg\"\n",
    "\n",
    "# Download the image\n",
    "response = requests.get(image_url)\n",
    "if response.status_code == 200:\n",
    "    img = Image.open(BytesIO(response.content))\n",
    "\n",
    "    # Resize to 256x256\n",
    "    img_resized = img.resize((256, 256))\n",
    "\n",
    "#     # Save the resized image\n",
    "#     img_resized.save(\"bee_resized.jpg\")\n",
    "#     print(\"Image resized and saved as bee_resized.jpg\")\n",
    "# else:\n",
    "#     print(\"Failed to download the image\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0837c308-da4e-4c54-a463-705f213de69c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sarmistha/miniconda3/envs/smol/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:818: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_attentions` is. When `return_dict_in_generate` is not `True`, `output_attentions` is ignored.\n",
      "  warnings.warn(\n",
      "`Qwen2VLRotaryEmbedding` can now be fully parameterized by passing the model config through the `config` argument. All other arguments will be removed in v4.46\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c5510cd3c2f49af8b919d61b4cf3df2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "\n",
    "device = 'cuda'\n",
    "\n",
    "# default: Load the model on the available device(s)\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-2B-Instruct\", torch_dtype=\"auto\", device_map=\"auto\",output_attentions=True\n",
    ")\n",
    "# We recommend enabling flash_attention_2 for better acceleration and memory saving, especially in multi-image and video scenarios.\n",
    "# model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "#     \"Qwen/Qwen2-VL-2B-Instruct\",\n",
    "#     torch_dtype=torch.bfloat16,\n",
    "#     attn_implementation=\"flash_attention_2\",\n",
    "#     device_map=\"auto\",\n",
    "# )\n",
    "\n",
    "# default processer\n",
    "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-2B-Instruct\")\n",
    "\n",
    "# The default range for the number of visual tokens per image in the model is 4-16384. You can set min_pixels and max_pixels according to your needs, such as a token count range of 256-1280, to balance speed and memory usage.\n",
    "# min_pixels = 256*28*28\n",
    "# max_pixels = 1280*28*28\n",
    "# processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-2B-Instruct\", min_pixels=min_pixels, max_pixels=max_pixels)\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"image\",\n",
    "                \"image\": img_resized#\"bee_resized.jpg\",\n",
    "            },\n",
    "            {\"type\": \"text\", \"text\": \"Describe this image.\"},\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "\n",
    "# Preparation for inference\n",
    "text = processor.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "image_inputs, video_inputs = process_vision_info(messages)\n",
    "inputs = processor(\n",
    "    text=[text],\n",
    "    images=image_inputs,\n",
    "    videos=video_inputs,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "inputs = inputs.to(\"cuda\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e862cc7d-bb56-4491-aa57-2eb5a04b2688",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.qwen2_vl.modeling_qwen2_vl import  VisionSdpaAttention,Qwen2VLSdpaAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7db6cb85-6eb4-475f-993c-ee65f81df28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_maps={}\n",
    "def hook_fn(module, input, output):\n",
    "    try:\n",
    "        attention_output, attention_weight = output\n",
    "        attention_maps[module.name] = attention_weight.to('cpu')\n",
    "    except:\n",
    "        attention_output, attention_weight,_ = output # output, attention maps , hidden states output\n",
    "        attention_maps[module.name] = attention_weight.to('cpu')\n",
    "\n",
    "   \n",
    "hooks = []\n",
    "for name, module in model.model.named_modules():\n",
    "    if isinstance(module,(Qwen2VLSdpaAttention)):\n",
    "        module.name = name  # Assign a name to the module for identification\n",
    "        hook = module.register_forward_hook(hook_fn)\n",
    "        hooks.append(hook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55835c0d-7449-458a-99ba-1fa6db6b8b6c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Qwen2VLModel is using Qwen2VLSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to the manual attention implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.\n",
      "The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.\n"
     ]
    }
   ],
   "source": [
    "output = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a83db05-bc5b-4a5a-b355-ba0d0173afbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "for hook in hooks:\n",
    "    hook.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3975596b-4926-4f0f-a3b9-80608aadf45e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.self_attn\n",
      "layers.1.self_attn\n",
      "layers.2.self_attn\n",
      "layers.3.self_attn\n",
      "layers.4.self_attn\n",
      "layers.5.self_attn\n",
      "layers.6.self_attn\n",
      "layers.7.self_attn\n",
      "layers.8.self_attn\n",
      "layers.9.self_attn\n",
      "layers.10.self_attn\n",
      "layers.11.self_attn\n",
      "layers.12.self_attn\n",
      "layers.13.self_attn\n",
      "layers.14.self_attn\n",
      "layers.15.self_attn\n",
      "layers.16.self_attn\n",
      "layers.17.self_attn\n",
      "layers.18.self_attn\n",
      "layers.19.self_attn\n",
      "layers.20.self_attn\n",
      "layers.21.self_attn\n",
      "layers.22.self_attn\n",
      "layers.23.self_attn\n",
      "layers.24.self_attn\n",
      "layers.25.self_attn\n",
      "layers.26.self_attn\n",
      "layers.27.self_attn\n"
     ]
    }
   ],
   "source": [
    "for key in attention_maps:\n",
    "    #print(attention_maps[key].shape)\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7c7c75d5-afe6-48b2-8321-49b80f115c5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c6db40142df44acb7b86ded5620a548",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(VBox(children=(Dropdown(description='Layer:', options=('layers.0.self_attn', 'layers.1.self_attâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "def plot_attention_head(attention_tensor, head_index, title_prefix=\"\"):\n",
    "    \"\"\"\n",
    "    Plots the attention map of a specific head from an attention tensor.\n",
    "    Args:\n",
    "        attention_tensor (torch.Tensor): The attention tensor of shape [batch_size, num_heads, seq_len, seq_len]\n",
    "        head_index (int): The index of the head to plot\n",
    "        title_prefix (str): Optional prefix for the plot title\n",
    "    \"\"\"\n",
    "    if len(attention_tensor.shape) != 4:\n",
    "        raise ValueError(\"The attention tensor must have 4 dimensions: [batch_size, num_heads, seq_len, seq_len]\")\n",
    "    \n",
    "    batch_size, num_heads, seq_len, _ = attention_tensor.shape\n",
    "    \n",
    "    if head_index >= num_heads:\n",
    "        raise ValueError(f\"head_index must be less than the number of heads ({num_heads})\")\n",
    "    \n",
    "    attention_map = attention_tensor[0, head_index]\n",
    "    \n",
    "    if attention_map.dtype == torch.bfloat16:\n",
    "        attention_map = attention_map.float()\n",
    "    \n",
    "    attention_map = attention_map.detach().cpu().numpy()\n",
    "    \n",
    "    plt.figure(figsize=(10, 10))\n",
    "    im = plt.imshow(attention_map, cmap='magma', vmin=0, vmax=1)\n",
    "    plt.colorbar(im, label='Attention Weight')\n",
    "    plt.title(f'{title_prefix} Attention Map for Head {head_index}')\n",
    "    plt.xlabel('Key Positions')\n",
    "    plt.ylabel('Query Positions')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "class AttentionVisualizer:\n",
    "    def __init__(self, attention_maps):\n",
    "        self.attention_maps = attention_maps\n",
    "        self.text_layers = sorted([k for k in attention_maps.keys() if k.startswith('layers.') and k.endswith('self_attn')])\n",
    "        \n",
    "        self.layer_dropdown = widgets.Dropdown(\n",
    "            options=self.text_layers,\n",
    "            description='Layer:',\n",
    "            style={'description_width': 'initial'}\n",
    "        )\n",
    "        \n",
    "        self.head_slider = widgets.IntSlider(\n",
    "            value=0,\n",
    "            min=0,\n",
    "            max=self._get_max_heads(self.text_layers[0]) - 1,\n",
    "            description='Head Index:',\n",
    "            style={'description_width': 'initial'}\n",
    "        )\n",
    "        \n",
    "        self.output = widgets.Output()\n",
    "        \n",
    "        self.layer_dropdown.observe(self.on_layer_change, names='value')\n",
    "        self.head_slider.observe(self.on_head_change, names='value')\n",
    "        \n",
    "        self._update_plot()\n",
    "    \n",
    "    def _get_max_heads(self, layer_key):\n",
    "        return self.attention_maps[layer_key].shape[1]\n",
    "    \n",
    "    def on_layer_change(self, change):\n",
    "        self.head_slider.max = self._get_max_heads(change['new']) - 1\n",
    "        self._update_plot()\n",
    "    \n",
    "    def on_head_change(self, _):\n",
    "        self._update_plot()\n",
    "    \n",
    "    def _update_plot(self):\n",
    "        with self.output:\n",
    "            self.output.clear_output(wait=True)\n",
    "            layer_key = self.layer_dropdown.value\n",
    "            attn_tensor = self.attention_maps[layer_key]\n",
    "            title_prefix = f'Text Model (Layer {layer_key.split(\".\")[1]}): '\n",
    "            plot_attention_head(attn_tensor, self.head_slider.value, title_prefix)\n",
    "    \n",
    "    def display(self):\n",
    "        controls = widgets.VBox([\n",
    "            self.layer_dropdown,\n",
    "            self.head_slider\n",
    "        ])\n",
    "        display(widgets.VBox([controls, self.output]))\n",
    "\n",
    "#Example usage:\n",
    "visualizer = AttentionVisualizer(attention_maps)\n",
    "visualizer.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c4a063-9bdc-4f13-8cc4-2519a3f5eebf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python SMOL",
   "language": "python",
   "name": "smol"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
